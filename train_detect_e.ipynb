{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.utils import np_utils\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dense\n",
    "from keras import backend as K\n",
    "\n",
    "class LeNet:\n",
    "    @staticmethod\n",
    "    def build(width, height, depth, classes):\n",
    "        # initialize the model\n",
    "        model = Sequential()\n",
    "        inputShape = (height, width, depth)\n",
    "\n",
    "        # if we are using \"channel_first\", update the input shape\n",
    "        if K.image_data_format() == \"channel_first\":\n",
    "            inputShape = (depth, height, width)\n",
    "\n",
    "        # first set of CONV => RELU => POOL layers\n",
    "        model.add(Conv2D(20, (5, 5), padding = \"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2)))\n",
    "\n",
    "        # second set of CONV => RELU => POOL layers\n",
    "        model.add(Conv2D(50, (5, 5), padding = \"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2)))\n",
    "\n",
    "        # first set of FC => RELU layers\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(500))\n",
    "        model.add(Activation(\"relu\"))\n",
    "\n",
    "        # softmax classififer\n",
    "        model.add(Dense(classes))\n",
    "        model.add(Activation(\"softmax\"))\n",
    "\n",
    "        # return the constructed network architecture\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# initialize the list of data and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "labels = []\n",
    "\n",
    "DATASET_PATH = 'dataset'\n",
    "MODEL_NAME = 'model.hdf5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the input images\n",
    "for imagePath in sorted(list(paths.list_images(DATASET_PATH))):\n",
    "    # load the image, pre-process it, and store it in the data list\n",
    "    image = cv2.imread(imagePath)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    image = imutils.resize(image, width = 28)\n",
    "    image = img_to_array(image)\n",
    "    data.append(image)\n",
    "\n",
    "    # extract the class label from the image path and update the label list\n",
    "    label = imagePath.split(os.path.sep)[-3]\n",
    "    label = \"E\" if label == \"positives\" else \"Others\"\n",
    "    labels.append(label)\n",
    "\n",
    "# scale the raw pixel intensities to the range [0, 1]\n",
    "data = np.array(data, dtype = \"float\") / 255.0\n",
    "labels = np.array(labels)\n",
    "\n",
    "# convert the labels from integers to vectors\n",
    "le = LabelEncoder().fit(labels)\n",
    "labels = np_utils.to_categorical(le.transform(labels), 2)\n",
    "\n",
    "# account for skew in the labeled data\n",
    "classTotals = labels.sum(axis = 0)\n",
    "classWeight = classTotals.max() / classTotals\n",
    "\n",
    "# partition the data into training and testing splits using 80% of the data\n",
    "# for training and remaining 20% for testing\n",
    "(trainX, testX, trainY, testY) = train_test_split(data, labels,\n",
    "    test_size = 0.20, stratify = labels, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-26 12:22:54.442818: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] compiling model...\")\n",
    "model = LeNet.build(width = 28, height = 28, depth = 1, classes = 2)\n",
    "model.compile(loss = \"binary_crossentropy\", optimizer = \"adam\",\n",
    "    metrics = [\"acc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training network...\n",
      "Epoch 1/15\n",
      "10/10 [==============================] - 2s 117ms/step - loss: 0.7072 - acc: 0.5149 - val_loss: 0.6861 - val_acc: 0.7881\n",
      "Epoch 2/15\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 0.6871 - acc: 0.6904 - val_loss: 0.6748 - val_acc: 0.5099\n",
      "Epoch 3/15\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.6438 - acc: 0.6440 - val_loss: 0.6116 - val_acc: 0.6490\n",
      "Epoch 4/15\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 0.6087 - acc: 0.6871 - val_loss: 0.5609 - val_acc: 0.7285\n",
      "Epoch 5/15\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 0.5218 - acc: 0.7765 - val_loss: 0.5843 - val_acc: 0.7086\n",
      "Epoch 6/15\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 0.5157 - acc: 0.7450 - val_loss: 0.4737 - val_acc: 0.7616\n",
      "Epoch 7/15\n",
      "10/10 [==============================] - 1s 92ms/step - loss: 0.4647 - acc: 0.8063 - val_loss: 0.4516 - val_acc: 0.7815\n",
      "Epoch 8/15\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 0.4306 - acc: 0.7997 - val_loss: 0.4499 - val_acc: 0.7550\n",
      "Epoch 9/15\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.4058 - acc: 0.8212 - val_loss: 0.4420 - val_acc: 0.7947\n",
      "Epoch 10/15\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 0.3780 - acc: 0.8411 - val_loss: 0.4550 - val_acc: 0.8013\n",
      "Epoch 11/15\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 0.4032 - acc: 0.8079 - val_loss: 0.4657 - val_acc: 0.7947\n",
      "Epoch 12/15\n",
      "10/10 [==============================] - 1s 87ms/step - loss: 0.3541 - acc: 0.8526 - val_loss: 0.4378 - val_acc: 0.8013\n",
      "Epoch 13/15\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.3233 - acc: 0.8659 - val_loss: 0.4473 - val_acc: 0.8079\n",
      "Epoch 14/15\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 0.3030 - acc: 0.8858 - val_loss: 0.4608 - val_acc: 0.8146\n",
      "Epoch 15/15\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 0.3055 - acc: 0.8725 - val_loss: 0.6164 - val_acc: 0.7550\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] training network...\")\n",
    "classWeight = {0: classWeight[0], 1: classWeight[1]}\n",
    "H = model.fit(trainX, trainY, validation_data = (testX, testY),\n",
    "    class_weight = classWeight, batch_size = 64, epochs = 15, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] evaluating network...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " not_smiling       0.89      0.57      0.69        74\n",
      "     smiling       0.69      0.94      0.80        77\n",
      "\n",
      "    accuracy                           0.75       151\n",
      "   macro avg       0.79      0.75      0.74       151\n",
      "weighted avg       0.79      0.75      0.75       151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] evaluating network...\")\n",
    "predictions = model.predict(testX, batch_size = 64)\n",
    "print(classification_report(testY.argmax(axis = 1),\n",
    "    predictions.argmax(axis = 1), target_names = le.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save the model to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot the training + testing loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'H' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/8h/f7npn2hx7d57jys2jypsw35r0000gq/T/ipykernel_3280/1299389300.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ggplot\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"train_loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val_loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"val_loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"acc\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"acc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'H' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, 15), H.history[\"loss\"], label = \"train_loss\")\n",
    "plt.plot(np.arange(0, 15), H.history[\"val_loss\"], label = \"val_loss\")\n",
    "plt.plot(np.arange(0, 15), H.history[\"acc\"], label = \"acc\")\n",
    "plt.plot(np.arange(0, 15), H.history[\"val_acc\"], label = \"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend()\n",
    "# plt.savefig(args[\"plot\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
